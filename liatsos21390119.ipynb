{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## web.py αρχείο\n",
    "Εισαγωγή βιβλιοθηκών"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import logging\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η δομή Trie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "trie = None\n",
    "final_tokens = None \n",
    "inverted_index = defaultdict(set)\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.doc_ids = set()\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, word, doc_id):\n",
    "        current = self.root\n",
    "        for char in word:\n",
    "            if char not in current.children:\n",
    "                current.children[char] = TrieNode()\n",
    "            current = current.children[char]\n",
    "        current.doc_ids.add(doc_id)\n",
    "\n",
    "    def search(self, word):\n",
    "        current = self.root\n",
    "        for char in word:\n",
    "            if char not in current.children:\n",
    "                return set()\n",
    "            current = current.children[char]\n",
    "        return current.doc_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ορισμός βοηθητικών συναρτήσεων\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "def default_run():\n",
    "    try:\n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('punkt_tab')\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        print(\"Setup complete\")\n",
    "    except Exception as e:\n",
    "        print(\"Error setting up : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 1 - Web Scrapping\n",
    "Η ρουτίνα scrape_wikipedia(URL) έχει ως όρισμα μια ιστοσελίδα από την wikipedia και τη κάνει parse. Έπειτα, καλεί την scrape_pargraphs()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_wikipedia(URL) :\n",
    "    r = requests.get(URL)\n",
    "    page = BeautifulSoup(r.text ,\"html.parser\")\n",
    "    scrape_paragraphs(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η ρουτίνα scrape_paragraphs(page) έχει ως όρισμα το page που παράγει η scrape_wikipedia και για κάθε παράγραφο του page, κρατάει μόνο το κείμενο που θα είναι χρήσιμο για μετέπειτα ανάλυση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_paragraphs(page):\n",
    "    try:\n",
    "        data = []\n",
    "        paragraph_id = 0  # Το ID για κάθε παράγραφο\n",
    "        for paragraph in page.select('p'):\n",
    "            text = paragraph.getText().strip()  \n",
    "            if text: \n",
    "                data.append(text)\n",
    "                tokens = text.lower().split()  # Tokenize\n",
    "                for token in tokens:\n",
    "                    inverted_index[token].add(paragraph_id)  # Ενημερώνουμε το ευρετήριο\n",
    "                paragraph_id += 1\n",
    "        save_to_csv(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing paragraphs: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η ρουτίνα save_to_csv(data) αποθηκεύει κάθε scraped παραγράφο στο αρχείο \"wikipedia_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data):\n",
    "    try:\n",
    "        df = pd.DataFrame(data,columns=[\"Paragraph Text\"])\n",
    "        df.to_csv('wikipedia_data.csv', index= False , encoding='utf-8')\n",
    "        print(\"Data saved Successfuly!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error Saving in csv File\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 2 - Text Processing\n",
    "Για κάθε παράγραφο που υπάρχει στο wikipedia_data.csv, γίνεται επεξεργασία με μεθόδους όπως stop-word removal, tokenize, stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αφενός, η ρουτίνα text_processing(CSVFile) ανακτά τις παραγράφους που είναι αποθηκευμένες στο csvFile και τις αποθηκεύει στην cleaned_data = [].\n",
    "Αφετέρου, οι πάραγραφοι καθαρίζονται όταν καλείται η cleaning_text_and_save(text).\n",
    "\n",
    "Η ρουτίνα cleaning_text_and_save περιέχει όλες τις εργασίες που πρέπει να γίνουν στο κείμενο.\n",
    "Πρώτα, αφαιρεί όλα τα σημεία στίξης, έπειτα τις κάνει tokenize και τέλος stem.\n",
    "Έπειτά, καλείται η create_inverted_index που αναλυέται στο βήμα 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(csvFile):\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csvFile, header=None)\n",
    "        cleaned_data = []\n",
    "\n",
    "        for text in df[0]:\n",
    "            if text:\n",
    "                cleaned_text = cleaning_text_and_save(text)\n",
    "                cleaned_data.append({cleaned_text})\n",
    "\n",
    "        cleaned_df = pd.DataFrame(cleaned_data)\n",
    "        save_new_text(cleaned_data)\n",
    "    except BaseException:\n",
    "        logging.exception(\"An exception was thrown\")\n",
    "\n",
    "\n",
    "def cleaning_text_and_save(text):\n",
    "    \n",
    "        #Stop-word removal\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        #Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "        stemmed_tokens = [ps.stem(word) for word in filtered_tokens]\n",
    "        cleaned_tokens = [re.sub(r'[^a-zA-Z0-9]', '', token) for token in stemmed_tokens]\n",
    "        #Clearing Text\n",
    "        cleaned_text = ''.join(stemmed_tokens)\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', cleaned_text)\n",
    "        #Final Tokens\n",
    "        global final_tokens\n",
    "        final_tokens = [token for token in cleaned_tokens if token]\n",
    "        create_inverted_index(final_tokens)\n",
    "        \n",
    "\n",
    "        return cleaned_text\n",
    "\n",
    "\n",
    "\n",
    "def save_new_text(cleaned_data):\n",
    "    try:\n",
    "        df = pd.DataFrame(cleaned_data)\n",
    "        df.to_csv('cleaned_text.csv', index= False , encoding='utf-8')\n",
    "        print(\"Cleaned data saved Successfuly!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error Saving in csv File\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 3 - Inverted Index και Υλοποιήση δομής δεδομένων\n",
    "Η create_inverted_index(final_tokens), παίρνει τα καθαρισμένα πλέον tokens που έβγαλε η cleaning_text_and_save και τα προσθέτει ένα ένα στο inverted index.\n",
    "\n",
    "Η inverted index είναι τύπου defaultdict αλλά χρησιμοποιείται σαν την dict. Μόνο που η defaultdict επιστρέφει set() (κενό σύνολο) σε περιπτώση που δε βρεθεί κάποιο κλειδί στους όρους της.\n",
    "\n",
    "Κάθε token προστίθεται στο inverted index.\n",
    "\n",
    "Και τέλος το inverted_index εξάγεται σαν txt file για την ευκολότερη επεξεργασία του αργότερα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(final_tokens):\n",
    "    global inverted_index \n",
    "    inverted_index = defaultdict(set) \n",
    "    \n",
    "    for idx, token in enumerate(final_tokens):\n",
    "        token = token.lower()  \n",
    "        if token: \n",
    "            inverted_index[token].add(idx)  \n",
    "    \n",
    "    with open(\"inverted_index.txt\", 'a', encoding='utf-8') as file:\n",
    "        for token, doc_ids in inverted_index.items():\n",
    "            doc_ids_str = ', '.join(map(str, doc_ids))  \n",
    "            file.write(f\"Token: {token} | Document IDs: {doc_ids_str}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η ρουτίνα insert_inverted_index_to_trie() μετρατρέπει το inverted_index σε trie. Για κάθε όρο, γίνεται insert το id του και ο ίδιος ο όρος."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_inverted_index_to_trie(inverted_index, trie):\n",
    "    for term, doc_ids in inverted_index.items():\n",
    "        for doc_id in doc_ids:\n",
    "            trie.insert(term, doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η συνάρτηση load_inverted_index_with_trie φορτώνει ένα αντεστραμμένο ευρετήριο (inverted index) από ένα αρχείο και το αποθηκεύει σε μια δομή Trie, για γρήγορη αναζήτηση λέξεων και των παραγράφων που εμφανίζονται.\n",
    "Επιστρέφει ένα αντικείμενο trie, διaφορετικά None σε περίπτωση λάθους."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inverted_index_with_trie(filename, num_paragraphs):\n",
    "    trie = Trie()\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' | ')  \n",
    "                token = parts[0].split(': ')[1]  \n",
    "                doc_ids_str = parts[1].split(': ')[1]  \n",
    "                \n",
    "                doc_ids = map(int, doc_ids_str.split(', '))\n",
    "                valid_doc_ids = [doc_id for doc_id in doc_ids if 0 <= doc_id < num_paragraphs]  \n",
    "                for doc_id in valid_doc_ids:\n",
    "                    trie.insert(token, doc_id)  \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the inverted index: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return trie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bήμα 4 : Μηχανή αναζήτησης \n",
    "Σε αυτό το βήμα, έχουν υλοποιηθεί συναρτήσεις που είναι αναγκαιές για την web διεπάφη.\n",
    "Το αρχείο της web διεπαφής θα αναλυθεί πιο κάτω.\n",
    "\n",
    "Η συνάρτηση search_in_inverted_index(token, inverted_index) επιστρέφει τα ID στα οποία υπάρχουν το token. Διαφορετικά γυρνάει κενό set()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_inverted_index(token, inverted_index):\n",
    "    return set(inverted_index.get(token, []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η συνάρτηση query_processing(query,trie,debug=False) είναι μια ρουτίνα η οποία επεξεργάζεται το query που δίνει ο user.\n",
    "\n",
    "Αφενός το query γίνεται tokenized.\n",
    "\n",
    "To query μπορεί να έχει τη μορφή :\n",
    "\n",
    "word1\n",
    "\n",
    "word1 AND word2\n",
    "\n",
    "word1 OR word2\n",
    "\n",
    "NOT word1\n",
    "\n",
    "word1 OR word2 AND word3 NOT word4\n",
    "\n",
    "  for token in tokens:\n",
    "        if debug:\n",
    "            print(f\"Processing token: {token}\")\n",
    "        \n",
    "        if token == \"and\":\n",
    "            current_operator = \"AND\"\n",
    "            continue\n",
    "        elif token == \"or\":\n",
    "            current_operator = \"OR\"\n",
    "            continue\n",
    "        elif token == \"not\":\n",
    "            current_operator = \"NOT\"\n",
    "            continue\n",
    "Σε αυτό το κομμάτι διαχωρίζεται το token από τις λογικές λέξεις. \n",
    "\n",
    "Έπειτα γίνεται μια αναζήτηση στο trie για κάθε token και αποθηκεύεται στη μεταβλητή doc_ids τα σύνολα όπου υπάρχουν τα tokens.\n",
    "\n",
    "Για κάθε token στο σύνολο doc_ids,  \n",
    " -> Aν ο τελεστής είναι \"AND\", το result_set επικαλύπτεται με τα κοινά doc_ids των δυο συνόλων (παλιό result_set και doc_ids)\n",
    "\n",
    " -> Aν ο τελεστής είναι \"OR\", το result_set επεκτείνεται για να περιλάβει όλα τα doc_ids που εμφανίζονται είτε στο παλιό σύνολο είτε στο νέο doc_ids.\n",
    "\n",
    " -> Αν ο τελεστής είναι \"NOT\", το result_set αφαιρεί τα doc_ids που εμφανίζονται στο doc_ids από το υπάρχον σύνολο.\n",
    "\n",
    " Επιστρέφεται το result_set έπειτα από την επεξεργασία."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_processing(query, trie, debug=False):\n",
    "    query = query.strip()\n",
    "    tokens = query.lower().split()\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Tokens from query:\", tokens)\n",
    "    \n",
    "    tokens_set = set(tokens)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Unique tokens set:\", tokens_set)\n",
    "    \n",
    "    result_set = None\n",
    "    current_operator = \"AND\"\n",
    "    \n",
    "    for token in tokens:\n",
    "        if debug:\n",
    "            print(f\"Processing token: {token}\")\n",
    "        \n",
    "        if token == \"and\":\n",
    "            current_operator = \"AND\"\n",
    "            continue\n",
    "        elif token == \"or\":\n",
    "            current_operator = \"OR\"\n",
    "            continue\n",
    "        elif token == \"not\":\n",
    "            current_operator = \"NOT\"\n",
    "            continue\n",
    "        \n",
    "        doc_ids = trie.search(token)  # Αναζητούμε το token στο trie\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Found doc_ids for '{token}': {doc_ids}\")\n",
    "        \n",
    "        if result_set is None:\n",
    "            result_set = doc_ids\n",
    "        elif current_operator == \"AND\":\n",
    "            result_set &= doc_ids\n",
    "        elif current_operator == \"OR\":\n",
    "            result_set |= doc_ids\n",
    "        elif current_operator == \"NOT\":\n",
    "            result_set -= doc_ids\n",
    "    return result_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H συνάρτηση search_in_invered_index αναζητά ένα token στο inverted_index.\n",
    "Αν το βρεί, γυρνάει τη θέση του.\n",
    "Αν όχι, γυρνάει κενό set()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_inverted_index(token,inverted_index):\n",
    "    token = token.lower()\n",
    "    print(\"Token in function :\", token)\n",
    "    if token in inverted_index:\n",
    "        return inverted_index[token]\n",
    "    else:\n",
    "        return set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Εδώ υλοποιούνται  οι αλγόριθμοι TF-IDF και BM25. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η συνάρτηση search_tfidf χρησιμοποιεί την τεχνική TF-IDF (Term Frequency-Inverse Document Frequency) για να αναζητήσει τις παραγράφους που είναι πιο συναφείς με ένα δοσμένο ερώτημα (query). Αρχικά, μετατρέπει όλες τις παραγράφους και το query σε χρησιμοποιώντας τον TfidfVectorizer, δημιουργώντας έναν πίνακα χαρακτηριστικών για κάθε παράγραφο και ένα vector για το query. Στη συνέχεια, υπολογίζει την ομοιότητα συνημιτόνου (cosine similarity) μεταξύ του query και κάθε παραγράφου για να μετρήσει πόσο κοντά είναι το query με κάθε παράγραφο. Οι παράγραφοι ταξινομούνται με βάση την ομοιότητά τους με το query, επιστρέφοντας τις πιο σχετικές πρώτες. Επιστρέφει μια λίστα με παραγράφους που έχουν θετική ομοιότητα με το query, ταξινομημένες από τις πιο σχετικές στις λιγότερο σχετικές."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tfidf(query,paragraphs,trie):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(paragraphs)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    similarity_scores = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    ranked_indices = similarity_scores.argsort()[::-1]\n",
    "    ranked_paragraphs = [paragraphs[i] for i in ranked_indices if similarity_scores[i] > 0]\n",
    "\n",
    "    return ranked_paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η συνάρτηση search_bm25 χρησιμοποιεί τον αλγόριθμο BM25 (Best Matching 25) για να αναζητήσει τις παραγράφους που είναι πιο συναφείς με το δοσμένο query. Αρχικά, η συνάρτηση μετατρέπει κάθε παράγραφο και το query σε λίστες από λέξεις (tokens), χρησιμοποιώντας την συνάρτηση word_tokenize από την βιβλιοθήκη nltk, και μετατρέπει τα κείμενα σε μικρούς χαρακτήρες (lowercase) για να εξασφαλίσει ότι η αναζήτηση είναι case-insensitive. Στη συνέχεια, δημιουργεί ένα αντικείμενο του τύπου BM25Okapi που υπολογίζει τα σκορ BM25 για κάθε παράγραφο, με βάση τη συχνότητα εμφάνισης των λέξεων του query σε κάθε παράγραφο και τη συχνότητα εμφάνισης των λέξεων στο σύνολο των παραγράφων. Τα σκορ αυτά δείχνουν πόσο συναφής είναι κάθε παράγραφος με το query. Οι παράγραφοι ταξινομούνται κατά φθίνουσα σειρά ομοιότητας (βάσει των σκορ BM25), και επιστρέφονται οι παράγραφοι με τα υψηλότερα σκορ, που υποδηλώνουν τη μεγαλύτερη συνάφεια με το query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_bm25(query, paragraphs, trie): \n",
    "    tokenized_paragraphs = [word_tokenize(paragraph.lower()) for paragraph in paragraphs]\n",
    "    tokenized_query = word_tokenize(query.lower())\n",
    "    bm25 = BM25Okapi(tokenized_paragraphs)\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "    ranked_paragraphs = [paragraphs[i] for i in ranked_indices if scores[i] > 0]\n",
    "    \n",
    "    return ranked_paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 5 - Αξιολόγηση συστήματος\n",
    "Η συνάρτηση calculate_metrics υπολογίζει τρεις βασικούς δείκτες απόδοσης για την αναζήτηση:\n",
    "\n",
    "Precision :Το ποσοστό των ανακτηθέντων εγγράφων που είναι πραγματικά σχετικά. Υπολογίζεται ως το πηλίκο του αριθμού των σχετικών εγγράφων που ανακτήθηκαν προς τον αριθμό των εγγράφων που ανακτήθηκαν.\n",
    "Recall : Το ποσοστό των σχετικών εγγράφων που ανακτήθηκαν σωστά. Υπολογίζεται ως το πηλίκο του αριθμού των σχετικών εγγράφων που ανακτήθηκαν προς τον αριθμό των συνολικών σχετικών εγγράφων.\n",
    "F1-Score: Η αρμονική μέση της ακρίβειας και της ανάκλησης. Είναι χρήσιμος όταν υπάρχει μια αδικία μεταξύ της ακρίβειας και της ανάκλησης, και προσφέρει μια συνολική μέτρηση της απόδοσης του συστήματος.\n",
    "Η συνάρτηση χρησιμοποιεί τις εξής διαδικασίες:\n",
    "\n",
    "Δημιουργεί δυαδικούς πίνακες (arrays) για την ακρίβεια (retrieved) και τη σχετικότητα (relevant) των εγγράφων, που περιέχουν 1 για κάθε σχετικό έγγραφο και 0 για τα υπόλοιπα.\n",
    "Υπολογίζει τα σκορ χρησιμοποιώντας τις βασικές μαθηματικές σχέσεις μεταξύ των ακρίβειας, ανάκλησης και F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(retrieved_docs, relevant_docs):\n",
    "    if not retrieved_docs and not relevant_docs:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    all_docs = retrieved_docs.union(relevant_docs)\n",
    "    if not all_docs:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    \n",
    "    \n",
    "    retrieved = np.array([1 if doc in retrieved_docs else 0 for doc in range(max(retrieved_docs.union(relevant_docs)) + 1)])\n",
    "    relevant = np.array([1 if doc in relevant_docs else 0 for doc in range(max(retrieved_docs.union(relevant_docs)) + 1)])\n",
    "    \n",
    "    precision = np.sum(retrieved & relevant) / np.sum(retrieved) if np.sum(retrieved) > 0 else 0\n",
    "    recall = np.sum(retrieved & relevant) / np.sum(relevant) if np.sum(relevant) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H συνάρτηση  calculate_map υπολογίζει την Μέση Ακρίβεια (MAP) για την αναζήτηση. Η MAP υπολογίζει την ακρίβεια σε κάθε θέση της λίστας αποτελεσμάτων, αλλά μόνο για τα σχετικά έγγραφα, και στη συνέχεια υπολογίζει τη μέση ακρίβεια αυτών των αποτελεσμάτων για όλα τα ερωτήματα:\n",
    "\n",
    "Για κάθε έγγραφο στη λίστα των ανακτηθέντων εγγράφων, αν το έγγραφο είναι σχετικό, υπολογίζεται η ακρίβεια στο σημείο (δηλαδή ο αριθμός των σχετικών εγγράφων που έχουν βρεθεί μέχρι αυτήν την θέση, δια του συνολικού αριθμού εγγράφων που έχουν ανακτηθεί μέχρι εκεί).\n",
    "Η MAP είναι η μέση ακρίβεια για όλα τα ερωτήματα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_map(retrieved_docs, relevant_docs):\n",
    "    if not relevant_docs:\n",
    "        return 0.0\n",
    "    avg_precision = 0\n",
    "    hits = 0\n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        if doc in relevant_docs:\n",
    "            hits += 1\n",
    "            avg_precision += hits / i\n",
    "    return avg_precision / len(relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η συνάρτηση evaluate_systems αξιολογεί το σύστημα αναζήτησης για ένα σύνολο από ερωτήματα:\n",
    "\n",
    "queries: Τα ερωτήματα που θέλουμε να αξιολογήσουμε.\n",
    "\n",
    "relevant_docs_set: Ένα σύνολο με τα σχετικά έγγραφα για κάθε ερώτημα.\n",
    "\n",
    "trie: Η δομή δεδομένων trie που χρησιμοποιείται για αναζήτηση.\n",
    "\n",
    "paragraphs: Οι παράγραφοι που περιέχουν τα έγγραφα προς αναζήτηση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_system(queries, relevant_docs_set, trie, paragraphs):\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        print(f\"Evaluating query: {query}\")\n",
    "        \n",
    "        retrieved_docs = query_processing(query, trie) or set()\n",
    "        \n",
    "        relevant_docs = relevant_docs_set.get(query, set())\n",
    "        \n",
    "      \n",
    "        precision, recall, f1 = calculate_metrics(retrieved_docs, relevant_docs)\n",
    "        map_score = calculate_map(list(retrieved_docs), relevant_docs)\n",
    "        \n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "            \"map\": map_score\n",
    "        })\n",
    "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}, MAP: {map_score:.2f}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Οι getters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getters    \n",
    "def get_tokens():\n",
    "    global final_tokens\n",
    "    return final_tokens\n",
    "\n",
    "\n",
    "def get_inverted_index():\n",
    "    global inverted_index\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η main και οι λειτουργίες της :\n",
    "\n",
    "Η default_run κατεβάζει τα πακέτα, καθορίζει το setup του stemmer κτλ.\n",
    "\n",
    "H scrape_wikipedia ξεκινάει όλη την διαδικασία της ανάκτησης πληροφορίας για το δωσμένο link.\n",
    "\n",
    "Η text_processing ξεκινάει την επεξεργασία κειμένου για το file \"wikipedia_data.csv\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lyprandos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Lyprandos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete\n",
      "Data saved Successfuly!\n",
      "Cleaned data saved Successfuly!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    default_run() #Περιλαμβάνει το κατέβασμα των πακέτων, το setup του stemmer κτλ\n",
    "    scrape_wikipedia(\"https://en.wikipedia.org/wiki/cristiano_ronaldo\")  #Bήμα 1\n",
    "    text_processing(\"wikipedia_data.csv\") #Βήμα 2\n",
    "    global inverted_index\n",
    "    inverted_index = get_inverted_index()\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## app.py - Η web διεπαφή."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ο παρακάτω κώδικας υλοποιεί μια εφαρμογή αναζήτησης που εκτελείται μέσω του web framework Flask. Η εφαρμογή επιτρέπει στους χρήστες να εισάγουν ερωτήματα αναζήτησης (queries) και να επιλέξουν διάφορους αλγόριθμους αναζήτησης (Boolean Retrieval, TF-IDF, και Okapi BM25) για να ανακτήσουν σχετικά παραγράμματα από μια βάση δεδομένων (ένα CSV αρχείο με παραγράφους από το Wikipedia). Ας το δούμε βήμα-βήμα:\n",
    "\n",
    "Αρχικοποίηση Flask και φόρτωση δεδομένων\n",
    "Flask app: Ο κώδικας δημιουργεί μια εφαρμογή Flask με την εντολή app = Flask(__name__), η οποία παρέχει τη δυνατότητα για web περιβάλλον και εξυπηρετεί αιτήματα HTTP.\n",
    "load_paragraphs(): Αυτή η συνάρτηση διαβάζει τις παραγράφους από το αρχείο wikipedia_data.csv και τις αποθηκεύει σε μια λίστα paragraphs.\n",
    "load_inverted_index(): Η συνάρτηση διαβάζει το αρχείο inverted_index.txt για να φορτώσει το αναστραμμένο ευρετήριο (inverted index), το οποίο είναι μια δομή δεδομένων που συνδέει λέξεις (tokens) με τα έγγραφα (παράγραφοι) που τις περιέχουν.\n",
    "load_inverted_index_with_trie(): Φορτώνει επίσης το αναστραμμένο ευρετήριο χρησιμοποιώντας τη δομή δεδομένων Trie, η οποία επιτρέπει γρήγορη αναζήτηση.\n",
    "Λειτουργία της αναζήτησης\n",
    "Η συνάρτηση search() είναι η κύρια διαδρομή (route) της εφαρμογής και εξυπηρετεί το αίτημα αναζήτησης. Υποστηρίζει τις μεθόδους GET και POST.\n",
    "Όταν η σελίδα φορτώνει για πρώτη φορά (GET), φορτώνει τις παραγράφους και το αναστραμμένο ευρετήριο, και ετοιμάζει την εφαρμογή για αναζητήσεις.\n",
    "Όταν ο χρήστης υποβάλει μια αναζήτηση (POST), η εφαρμογή παίρνει το query (ερώτημα) και τον αλγόριθμο που επέλεξε ο χρήστης για την αναζήτηση. Στη συνέχεια, η εφαρμογή αναζητά τα σχετικά παραγράφους χρησιμοποιώντας τον επιλεγμένο αλγόριθμο. Επιλογή αλγορίθμου και αναζήτηση\n",
    "Υπάρχουν τρεις αλγόριθμοι αναζήτησης που υποστηρίζονται από την εφαρμογή:\n",
    "\n",
    "Boolean Retrieval: Χρησιμοποιείται για την αναζήτηση με βάση λογικούς τελεστές (AND, OR, NOT). Ο χρήστης μπορεί να αναζητήσει λέξεις και να βρει τα σχετικά έγγραφα μέσω του αναστραμμένου ευρετηρίου.\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency): Αυτή η μέθοδος χρησιμοποιεί τις συχνότητες λέξεων σε σχέση με το πόσο κοινές είναι σε όλο το σύνολο των παραγράφων για να υπολογίσει την σχετικότητα κάθε παραγράφου για το ερώτημα.\n",
    "Okapi BM25: Αυτή είναι μια στατιστική μέθοδος αναζήτησης που υπολογίζει την σχετικότητα κάθε παραγράφου με βάση την εμφάνιση των λέξεων και ένα σύνολο ρυθμιζόμενων παραμέτρων.\n",
    "Αξιολόγηση Απόδοσης\n",
    "Όταν ολοκληρωθεί η αναζήτηση, η εφαρμογή υπολογίζει την απόδοση του συστήματος αναζήτησης μέσω μετρικών αξιολόγησης όπως:\n",
    "Precision: Το ποσοστό των ανακτηθέντων εγγράφων που είναι πραγματικά σχετικά.\n",
    "Recall: Το ποσοστό των σχετικών εγγράφων που ανακτήθηκαν.\n",
    "F1-Score: Ο αρμονικός μέσος των precision και recall.\n",
    "MAP (Mean Average Precision): Ο μέσος όρος της ακρίβειας σε κάθε σημείο της λίστας αποτελεσμάτων.\n",
    "Αυτές οι μετρήσεις υπολογίζονται χρησιμοποιώντας τις συναρτήσεις calculate_metrics και calculate_map από το module web.\n",
    "Αποστολή Αποτελεσμάτων στην Σελίδα\n",
    "Μετά την επεξεργασία της αναζήτησης και την αξιολόγηση της απόδοσης, τα αποτελέσματα της αναζήτησης (τα σχετικά παραγράμματα) και οι μετρικές απόδοσης (precision, recall, F1, MAP) επιστρέφονται στον χρήστη μέσω της λειτουργίας render_template_string.\n",
    "Το αποτέλεσμα εμφανίζεται στην ιστοσελίδα και περιλαμβάνει μια λίστα με τις παραγράφους που βρέθηκαν και τα αντίστοιχα αποτελέσματα των μετρικών.\n",
    "Εκκίνηση του Web Server\n",
    "Στο τέλος, η εφαρμογή εκκινεί τον web server με την εντολή app.run(debug=True), επιτρέποντας στην εφαρμογή να λειτουργεί και να ανταποκρίνεται στα αιτήματα αναζήτησης από το χρήστη."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lyprandos\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, render_template_string\n",
    "import web\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "final_tokens = web.get_tokens() \n",
    "\n",
    "def load_paragraphs():\n",
    "    paragraphs = []\n",
    "    with open('wikipedia_data.csv', 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            paragraphs.append(line.strip())\n",
    "    return paragraphs\n",
    "\n",
    "#def search_paragraphs(result_set, inverted_index, paragraphs):\n",
    "    #result_paragraphs = []\n",
    "\n",
    "    for doc_id in result_set:\n",
    "        if 0 <= doc_id < len(paragraphs):\n",
    "            result_paragraphs.append(paragraphs[doc_id])\n",
    "        else:\n",
    "            print(f\"Invalid doc_id: {doc_id} is out of range.\")\n",
    "    \n",
    "    if not result_paragraphs:\n",
    "        return [\"No valid paragraphs found.\"]\n",
    "    return result_paragraphs\n",
    "\n",
    "\n",
    "\n",
    "def load_inverted_index(filename, num_paragraphs):\n",
    "    inverted_index = defaultdict(set)\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(' | ')  \n",
    "            token = parts[0].split(': ')[1] \n",
    "            doc_ids_str = parts[1].split(': ')[1]  \n",
    "            \n",
    "            doc_ids = map(int, doc_ids_str.split(', '))\n",
    "            \n",
    "            for doc_id in doc_ids:\n",
    "                if 0 <= doc_id < num_paragraphs:  \n",
    "                    inverted_index[token].add(doc_id)\n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def search():\n",
    "    paragraphs = load_paragraphs()\n",
    "    num_paragraphs = len(paragraphs)\n",
    "    inverted_index = load_inverted_index('inverted_index.txt',num_paragraphs)  \n",
    "    trie = web.load_inverted_index_with_trie('inverted_index.txt', num_paragraphs)\n",
    "    result = None\n",
    "    query = \"\"\n",
    "    algorithm = \"boolean\" #default algorithm \n",
    "    queries = {\"ronaldo\",\"cristiano\",\"ball\",\"instagram\",\"cr7\",\"goal\",\"scorer\",\"foot\",\"messi\",\"cup\",\"champion\",\"legend\"}\n",
    "    retrieved_docs = set()\n",
    "    relevant_docs_set = {}\n",
    "    for query in queries:\n",
    "        if query in inverted_index:\n",
    "            relevant_docs_set[query] = inverted_index[query]\n",
    "        else:\n",
    "            relevant_docs_set[query] = set()  \n",
    "    print(relevant_docs_set)\n",
    "\n",
    "    if request.method == \"POST\":\n",
    "        \n",
    "        query = request.form.get(\"query\", \"\").strip()\n",
    "        algorithm = request.form.get(\"algorithm\",\"boolean\")\n",
    "        \n",
    "        if algorithm == \"boolean\":\n",
    "            print(algorithm)\n",
    "            retrieved_docs = web.query_processing(query, trie, debug=False)\n",
    "            result_set = web.query_processing(query, trie, debug=False)\n",
    "            result = [paragraphs[doc_id] for doc_id in result_set if 0 <= doc_id < len(paragraphs)]\n",
    "        elif algorithm == \"tfidf\":\n",
    "            print(algorithm)\n",
    "            retrieved_docs = set(range(len(paragraphs)))\n",
    "            result = web.search_tfidf(query,paragraphs,trie)\n",
    "        elif algorithm == \"bm25\":\n",
    "            print(algorithm)\n",
    "            retrieved_docs = set(range(len(paragraphs)))\n",
    "            result = web.search_bm25(query,paragraphs,trie) \n",
    "\n",
    "        relevant_docs = relevant_docs_set.get(query, set())\n",
    "        precision, recall, f1 = web.calculate_metrics(retrieved_docs, relevant_docs)\n",
    "        map_score = web.calculate_map(list(retrieved_docs), relevant_docs)\n",
    "\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}, MAP: {map_score:.2f}\")\n",
    "\n",
    "    return render_template_string(\"\"\"\n",
    "    <h1>Search Paragraphs</h1>\n",
    "    <form method=\"POST\">\n",
    "    <input type=\"text\" name=\"query\" placeholder=\"Enter a word to search\" value=\"{{ query }}\">\n",
    "    <label for=\"algorithm\">Choose an algorithm:</label>\n",
    "    <select name=\"algorithm\">\n",
    "        <option value=\"boolean\">Boolean Retrieval</option>\n",
    "        <option value=\"tfidf\">TF-IDF (VSM)</option>\n",
    "        <option value=\"bm25\">Okapi BM25</option>\n",
    "    </select>\n",
    "    <button type=\"submit\">Search</button>\n",
    "</form>\n",
    "    \n",
    "    {% if result %}\n",
    "        <h2>Search Results for \"{{ query }}\":</h2>\n",
    "        <ul>\n",
    "            {% if result == \"No words found.\" %}\n",
    "                <li>No paragraphs found for the given query.</li>\n",
    "            {% else %}\n",
    "                {% for paragraph in result %}\n",
    "                    <li>{{ paragraph }}</li>\n",
    "                {% endfor %}\n",
    "            {% endif %}\n",
    "        </ul>\n",
    "    {% endif %}\n",
    "    \"\"\", query=query, result=result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ενδεικτικά τρεξίματα :\n",
    "Τα παραδείγματα θα είναι σε μορφή text διότι ανακτούνται από το web.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  user's query : ronaldo\n",
    "\n",
    "terminal output : \n",
    "\n",
    "boleean\n",
    "\n",
    "result set {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, \n",
    "38, 39, 40, 41, 42, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 63, 64, 66, 68, 69, 71, 74, 76, 77, 78, 79, 81, 82, 84, 87, 88, 89, 91, 92, 93, 94, 97, 103}\n",
    "\n",
    "Query: ronaldo\n",
    "\n",
    "Precision: 1.00, Recall: 1.00, F1-Score: 1.00, MAP: 1.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. user's query : ronaldo and cr7\n",
    "\n",
    "terminal output :\n",
    "\n",
    "boolean\n",
    "\n",
    "result set {40, 5, 59, 21}\n",
    "\n",
    "Query: ronaldo and  cr7\n",
    "\n",
    "Precision: 0.00, Recall: 0.00, F1-Score: 0.00, MAP: 0.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. user's query : cup\n",
    "\n",
    "terminal output :\n",
    "\n",
    "tfidf\n",
    "\n",
    "Query: cup\n",
    "\n",
    "Precision: 0.20, Recall: 1.00, F1-Score: 0.34, MAP: 0.42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. user's query : instagram\n",
    "\n",
    "terminal output : \n",
    "\n",
    "bm25\n",
    "\n",
    "Query: instagram\n",
    "\n",
    "Precision: 0.03, Recall: 1.00, F1-Score: 0.06, MAP: 0.06\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
